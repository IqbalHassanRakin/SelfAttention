{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Install Necessary Libraries**"
      ],
      "metadata": {
        "id": "LLNGfHgz34RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy torch seaborn matplotlib nltk"
      ],
      "metadata": {
        "id": "gNQReM2XMp8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Define Self-Attention Mechanism**"
      ],
      "metadata": {
        "id": "vJHmI7UD3_UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size must be divisible by heads\"\n",
        "\n",
        "        self.values = torch.nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = torch.nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = torch.nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = torch.nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / self.scale\n",
        "        if mask is not None:\n",
        "            energy -= 1e10 * mask\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=3)\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out, attention"
      ],
      "metadata": {
        "id": "BODRU6wA4F3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Prepare Sample Data**\n",
        "\n"
      ],
      "metadata": {
        "id": "UCagAqVe4Syc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The cat sat on the mat\"\n",
        "\n",
        "embed_size = 16\n",
        "heads = 4\n",
        "tokens = sentence.split()\n",
        "seq_length = len(tokens)\n",
        "\n",
        "def create_random_embeddings(tokens, embed_size):\n",
        "    embeddings = torch.rand(len(tokens), embed_size)\n",
        "    return embeddings\n",
        "\n",
        "values = create_random_embeddings(tokens, embed_size).unsqueeze(0)\n",
        "keys = values.clone()\n",
        "queries = values.clone()\n",
        "mask = None"
      ],
      "metadata": {
        "id": "GsG_BSLB4NAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Run Self-Attention**"
      ],
      "metadata": {
        "id": "1yGMCKMH4X-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self_attention = SelfAttention(embed_size, heads)\n",
        "\n",
        "output, attention_scores = self_attention(values, keys, queries, mask)"
      ],
      "metadata": {
        "id": "liiwY7FQ4eeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Create Word Labels and Plot Heatmap**"
      ],
      "metadata": {
        "id": "663kvbBp4iQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "word_labels = tokens\n",
        "\n",
        "attention_scores = attention_scores[0, 0].detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    attention_scores,\n",
        "    cmap=\"viridis\",\n",
        "    annot=True,\n",
        "    fmt=\".2f\",\n",
        "    xticklabels=word_labels,\n",
        "    yticklabels=word_labels\n",
        ")\n",
        "plt.title(\"Self-Attention Scores Heatmap\")\n",
        "plt.xlabel(\"Keys (Words)\")\n",
        "plt.ylabel(\"Queries (Words)\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t_LapruS4mgY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}